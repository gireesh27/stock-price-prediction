{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single notebook cell: Load from MongoDB (last 5 days), feature engineering, tuning, stacking, save, optional auto-retrain\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "import gridfs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from dotenv import load_dotenv\n",
        "from pymongo import MongoClient\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.options.mode.chained_assignment = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# CONFIG\n",
        "# ---------------------------\n",
        "load_dotenv()\n",
        "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
        "DB_NAME = os.getenv(\"DB_NAME\", \"stock-price-prediction\")\n",
        "\n",
        "STOCK_LIST = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\",\n",
        "              \"META\", \"NFLX\", \"NVDA\", \"IBM\", \"ORCL\"]\n",
        "\n",
        "LOOKBACK_DAYS = 30  # last 30 days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def connect_mongo(uri):\n",
        "    if not uri:\n",
        "        raise RuntimeError(\"MONGODB_URI not provided in environment\")\n",
        "    client = MongoClient(uri)\n",
        "    return client\n",
        "\n",
        "client = connect_mongo(MONGODB_URI)\n",
        "db = client[DB_NAME]\n",
        "fs = gridfs.GridFS(db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------\n",
        "# Functions for saving/loading model in GridFS\n",
        "# ------------------------------------------------------\n",
        "def save_model_to_gridfs(model, scaler):\n",
        "    import io\n",
        "\n",
        "    # delete old ones\n",
        "    for f in fs.find({\"filename\": \"best_stock_model.pkl\"}):\n",
        "        fs.delete(f._id)\n",
        "    for f in fs.find({\"filename\": \"scaler.pkl\"}):\n",
        "        fs.delete(f._id)\n",
        "\n",
        "    # Save model\n",
        "    model_bytes = io.BytesIO()\n",
        "    joblib.dump(model, model_bytes)\n",
        "    model_bytes.seek(0)\n",
        "    fs.put(model_bytes.read(), filename=\"best_stock_model.pkl\")\n",
        "\n",
        "    # Save scaler\n",
        "    scaler_bytes = io.BytesIO()\n",
        "    joblib.dump(scaler, scaler_bytes)\n",
        "    scaler_bytes.seek(0)\n",
        "    fs.put(scaler_bytes.read(), filename=\"scaler.pkl\")\n",
        "\n",
        "    print(\"\\n✔ Model + Scaler saved to MongoDB GridFS\")\n",
        "\n",
        "def load_model_from_gridfs():\n",
        "    import io\n",
        "\n",
        "    model_file = fs.find_one({\"filename\": \"best_stock_model.pkl\"})\n",
        "    scaler_file = fs.find_one({\"filename\": \"scaler.pkl\"})\n",
        "\n",
        "    if not model_file or not scaler_file:\n",
        "        raise RuntimeError(\"Model/Scaler not found in GridFS\")\n",
        "\n",
        "    model = joblib.load(io.BytesIO(model_file.read()))\n",
        "    scaler = joblib.load(io.BytesIO(scaler_file.read()))\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# RSI\n",
        "# ------------------------------------------------------\n",
        "def compute_rsi(series, period=14):\n",
        "    delta = series.diff()\n",
        "    gain = delta.clip(lower=0).rolling(period).mean()\n",
        "    loss = -delta.clip(upper=0).rolling(period).mean()\n",
        "    rs = gain / (loss.replace(0, 1e-8))\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Load last N days from MongoDB\n",
        "# ------------------------------------------------------\n",
        "def load_last_n_days_from_mongo(client, db_name, symbols, days=30):\n",
        "    db = client[db_name]\n",
        "    now_utc = datetime.utcnow()\n",
        "    start = now_utc - timedelta(days=days)\n",
        "\n",
        "    frames = []\n",
        "    for sym in symbols:\n",
        "        coll = db[sym.upper()]\n",
        "        cursor = coll.find({\"Date\": {\"$gte\": start}})\n",
        "        df = pd.DataFrame(list(cursor))\n",
        "        if df.empty:\n",
        "            print(f\"No recent data for {sym}\")\n",
        "            continue\n",
        "\n",
        "        if \"_id\" in df.columns and \"Date\" not in df.columns:\n",
        "            df[\"Date\"] = df[\"_id\"]\n",
        "\n",
        "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "        df = df.sort_values(\"Date\").reset_index(drop=True)\n",
        "        df[\"symbol\"] = sym.upper()\n",
        "\n",
        "        expected = [\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"Adj_Close\",\"symbol\"]\n",
        "        for col in expected:\n",
        "            if col not in df:\n",
        "                df[col] = np.nan\n",
        "\n",
        "        frames.append(df[expected])\n",
        "\n",
        "    if not frames:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_all = pd.concat(frames).sort_values([\"symbol\",\"Date\"]).reset_index(drop=True)\n",
        "    return df_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# UPDATED Feature Engineering (Dynamic indicators)\n",
        "# ---------------------------\n",
        "def add_features(df_all):\n",
        "    df_all = df_all.copy()\n",
        "\n",
        "    def fe_group(g):\n",
        "        g = g.sort_values(\"Date\").reset_index(drop=True)\n",
        "        rows = len(g)\n",
        "\n",
        "        print(f\"[{g['symbol'].iloc[0]}] Rows available = {rows}\")\n",
        "\n",
        "        # ================================\n",
        "        # PRIORITY 1 → FULL INDICATORS\n",
        "        # Need 50 rows (MA50 requires 50)\n",
        "        # ================================\n",
        "        if rows >= 50:\n",
        "            print(\" → Using FULL indicators (RSI14, MA20, MA50)...\")\n",
        "\n",
        "            g[\"RSI14\"] = compute_rsi(g[\"Close\"], 14)\n",
        "            g[\"MA20\"] = g[\"Close\"].rolling(20).mean()\n",
        "            g[\"MA50\"] = g[\"Close\"].rolling(50).mean()\n",
        "\n",
        "            # lag features\n",
        "            g[\"Close_1\"] = g[\"Close\"].shift(1)\n",
        "            g[\"Close_2\"] = g[\"Close\"].shift(2)\n",
        "            g[\"Close_3\"] = g[\"Close\"].shift(3)\n",
        "            g[\"Close_5\"] = g[\"Close\"].shift(5)\n",
        "\n",
        "        # ================================\n",
        "        # PRIORITY 2 → MEDIUM INDICATORS\n",
        "        # Need ≥ 20 rows\n",
        "        # ================================\n",
        "        elif rows >= 20:\n",
        "            print(\" → Using MEDIUM indicators (RSI7, MA10)...\")\n",
        "\n",
        "            g[\"RSI7\"] = compute_rsi(g[\"Close\"], 7)\n",
        "            g[\"MA10\"] = g[\"Close\"].rolling(10).mean()\n",
        "\n",
        "            g[\"Close_1\"] = g[\"Close\"].shift(1)\n",
        "            g[\"Close_2\"] = g[\"Close\"].shift(2)\n",
        "            g[\"Close_3\"] = g[\"Close\"].shift(3)\n",
        "\n",
        "        # ================================\n",
        "        # PRIORITY 3 → SMALL INDICATORS\n",
        "        # Need ≥ 14 rows\n",
        "        # ================================\n",
        "        elif rows >= 14:\n",
        "            print(\" → Using SMALL indicators (RSI7 only + lags)...\")\n",
        "\n",
        "            g[\"RSI7\"] = compute_rsi(g[\"Close\"], 7)\n",
        "\n",
        "            g[\"Close_1\"] = g[\"Close\"].shift(1)\n",
        "            g[\"Close_2\"] = g[\"Close\"].shift(2)\n",
        "\n",
        "        # ================================\n",
        "        # PRIORITY 4 → MINIMAL FEATURES\n",
        "        # Need ≥ 7 rows\n",
        "        # Only lag features\n",
        "        # ================================\n",
        "        elif rows >= 7:\n",
        "            print(\" → Using MINIMAL features (lags only)...\")\n",
        "\n",
        "            g[\"Close_1\"] = g[\"Close\"].shift(1)\n",
        "            g[\"Close_2\"] = g[\"Close\"].shift(2)\n",
        "\n",
        "        # ================================\n",
        "        # PRIORITY 5 → TOO LITTLE DATA\n",
        "        # Skip this stock entirely\n",
        "        # ================================\n",
        "        else:\n",
        "            print(\" - Not enough data (< 7 rows), skipping this symbol\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Target: next candle direction\n",
        "        g[\"target\"] = (g[\"Close\"].shift(-1) > g[\"Close\"]).astype(int)\n",
        "\n",
        "        # Drop unusable rows (indicator warmups)\n",
        "        g = g.dropna().reset_index(drop=True)\n",
        "        return g\n",
        "\n",
        "    # Apply per symbol\n",
        "    df_fe = df_all.groupby(\"symbol\", group_keys=False).apply(fe_group)\n",
        "\n",
        "    print(\"Final feature-engineered shape:\", df_fe.shape)\n",
        "    return df_fe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Training pipeline (GridFS version)\n",
        "# ---------------------------\n",
        "import io\n",
        "\n",
        "\n",
        "def train_and_save_model(df_all, fs, feature_cols=None):\n",
        "    if df_all.empty:\n",
        "        print(\"No data available to train.\")\n",
        "        return None, None\n",
        "\n",
        "    if feature_cols is None:\n",
        "        feature_cols = [\n",
        "            'Open','High','Low','Close','Volume',\n",
        "            'return_1','return_3','return_7',\n",
        "            'sma_5','sma_10','sma_20','ema_10','ema_20',\n",
        "            'vol_5','vol_10','mom_3','mom_7',\n",
        "            'vol_change','vol_ratio_5','rsi_14',\n",
        "            'month','dayofweek','is_quarter_end',\n",
        "            'close_lag_1','close_lag_2','close_lag_3','close_lag_5',\n",
        "            'vol_lag_1','vol_lag_2','vol_lag_3','vol_lag_5'\n",
        "        ]\n",
        "\n",
        "    # ensure all features exist\n",
        "    missing = [c for c in feature_cols if c not in df_all.columns]\n",
        "    if missing:\n",
        "        print(\"Warning - missing features, dropping:\", missing)\n",
        "        feature_cols = [c for c in feature_cols if c in df_all.columns]\n",
        "\n",
        "    X = df_all[feature_cols].copy()\n",
        "    y = df_all[\"target\"].copy()\n",
        "\n",
        "    # sort by time\n",
        "    df_all = df_all.sort_values(\"Date\").reset_index(drop=True)\n",
        "    split_index = int(len(df_all) * 0.8)\n",
        "\n",
        "    X_train = X.iloc[:split_index]\n",
        "    X_valid = X.iloc[split_index:]\n",
        "    y_train = y.iloc[:split_index]\n",
        "    y_valid = y.iloc[split_index:]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "    # Models & params\n",
        "    models_to_try = {}\n",
        "    param_grids = {}\n",
        "\n",
        "    models_to_try['rf'] = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "    param_grids['rf'] = {\n",
        "        'n_estimators': [200, 400],\n",
        "        'max_depth': [4, 6, 8],\n",
        "        'class_weight': [None, 'balanced']\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        from xgboost import XGBClassifier\n",
        "        models_to_try['xgb'] = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
        "        param_grids['xgb'] = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [3, 5],\n",
        "            'learning_rate': [0.01, 0.05, 0.1],\n",
        "            'subsample': [0.8, 1.0]\n",
        "        }\n",
        "    except:\n",
        "        print(\"xgboost not available - skipping\")\n",
        "\n",
        "    try:\n",
        "        from lightgbm import LGBMClassifier\n",
        "        models_to_try['lgbm'] = LGBMClassifier(random_state=42, n_jobs=-1)\n",
        "        param_grids['lgbm'] = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [-1, 6],\n",
        "            'learning_rate': [0.01, 0.05, 0.1]\n",
        "        }\n",
        "    except:\n",
        "        print(\"lightgbm not available - skipping\")\n",
        "\n",
        "    print(\"Models available:\", list(models_to_try.keys()))\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "    best_estimators = {}\n",
        "    validation_aucs = {}\n",
        "\n",
        "    # Tune models\n",
        "    for name, model in models_to_try.items():\n",
        "        print(f\"\\nTuning {name} ...\")\n",
        "        grid = RandomizedSearchCV(\n",
        "            model,\n",
        "            param_grids.get(name, {}),\n",
        "            n_iter=8,\n",
        "            scoring='roc_auc',\n",
        "            cv=tscv,\n",
        "            n_jobs=-1,\n",
        "            random_state=42\n",
        "        )\n",
        "        grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "        best = grid.best_estimator_\n",
        "        best_estimators[name] = best\n",
        "\n",
        "        # predict proba\n",
        "        if hasattr(best, \"predict_proba\"):\n",
        "            y_valid_proba = best.predict_proba(X_valid_scaled)[:, 1]\n",
        "        else:\n",
        "            try:\n",
        "                y_valid_proba = best.decision_function(X_valid_scaled)\n",
        "                y_valid_proba = (y_valid_proba - y_valid_proba.min()) / (y_valid_proba.max() - y_valid_proba.min() + 1e-8)\n",
        "            except:\n",
        "                y_valid_proba = best.predict(X_valid_scaled)\n",
        "\n",
        "        auc = roc_auc_score(y_valid, y_valid_proba)\n",
        "        validation_aucs[name] = auc\n",
        "\n",
        "        print(f\" Best params ({name}):\", grid.best_params_)\n",
        "        print(f\" Validation AUC ({name}): {auc:.4f}\")\n",
        "\n",
        "    # Choose top models for stacked ensemble\n",
        "    sorted_models = sorted(validation_aucs.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_names = [name for name, _ in sorted_models[:3]]\n",
        "\n",
        "    print(\"\\nStacking:\", top_names)\n",
        "\n",
        "    estimators_for_stack = [(name, best_estimators[name]) for name in top_names]\n",
        "\n",
        "    stack = StackingClassifier(\n",
        "        estimators=estimators_for_stack,\n",
        "        final_estimator=LogisticRegression(),\n",
        "        cv=5,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining stacking model...\")\n",
        "    stack.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Validate stacking model\n",
        "    y_valid_proba_stack = stack.predict_proba(X_valid_scaled)[:, 1]\n",
        "    auc_stack = roc_auc_score(y_valid, y_valid_proba_stack)\n",
        "    print(f\"\\nStacking Validation AUC: {auc_stack:.4f}\")\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # SAVE MODEL + SCALER TO MONGODB GRIDFS\n",
        "    # ----------------------------------------\n",
        "\n",
        "    # Remove old files\n",
        "    for file in fs.find({\"filename\": \"best_stock_model.pkl\"}): fs.delete(file._id)\n",
        "    for file in fs.find({\"filename\": \"scaler.pkl\"}): fs.delete(file._id)\n",
        "\n",
        "    # Save new files\n",
        "    model_bytes = io.BytesIO()\n",
        "    scaler_bytes = io.BytesIO()\n",
        "\n",
        "    joblib.dump(stack, model_bytes)\n",
        "    joblib.dump(scaler, scaler_bytes)\n",
        "\n",
        "    model_bytes.seek(0)\n",
        "    scaler_bytes.seek(0)\n",
        "\n",
        "    fs.put(model_bytes.read(), filename=\"best_stock_model.pkl\")\n",
        "    fs.put(scaler_bytes.read(), filename=\"scaler.pkl\")\n",
        "\n",
        "    print(\"\\n Model + scaler saved to MongoDB GridFS\")\n",
        "\n",
        "    return stack, scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Orchestration: load -> fe -> train (GridFS version)\n",
        "# ---------------------------\n",
        "def retrain_from_mongo(days=LOOKBACK_DAYS):\n",
        "    # Connect to MongoDB\n",
        "    client = connect_mongo(MONGODB_URI)\n",
        "    db = client[DB_NAME]\n",
        "    fs = gridfs.GridFS(db)\n",
        "\n",
        "    # Load recent data\n",
        "    raw = load_last_n_days_from_mongo(client, DB_NAME, STOCK_LIST, days=days)\n",
        "    if raw.empty:\n",
        "        print(\"No data loaded from MongoDB. Aborting retrain.\")\n",
        "        return None, None\n",
        "\n",
        "    # Feature engineering\n",
        "    df_fe = add_features(raw)\n",
        "    print(\"Feature-engineered dataset shape:\", df_fe.shape)\n",
        "\n",
        "    # Train & save model -> GridFS\n",
        "    stack, scaler = train_and_save_model(df_fe, fs)\n",
        "\n",
        "    print(\"Retraining complete.\")\n",
        "    return stack, scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Optional: Background retrain scheduler (every N days)\n",
        "# ---------------------------\n",
        "def start_auto_retrain(every_n_days=30, run_immediately=False):\n",
        "    \"\"\"\n",
        "    Starts a background scheduler that runs retrain_from_mongo every `every_n_days`.\n",
        "    Works with GridFS model saving. Not recommended on short-lived hosting\n",
        "    (Render free plan resets dynos).\n",
        "    \"\"\"\n",
        "    from apscheduler.schedulers.background import BackgroundScheduler\n",
        "\n",
        "    scheduler = BackgroundScheduler()\n",
        "\n",
        "    scheduler.add_job(\n",
        "        func=lambda: retrain_from_mongo(days=LOOKBACK_DAYS),\n",
        "        trigger='interval',\n",
        "        days=every_n_days,\n",
        "        next_run_time=(datetime.now() if run_immediately else None)\n",
        "    )\n",
        "\n",
        "    scheduler.start()\n",
        "\n",
        "    print(f\"[Scheduler] Auto-retrain scheduled every {every_n_days} days.\")\n",
        "    if run_immediately:\n",
        "        print(\"[Scheduler] First retrain will run immediately.\")\n",
        "\n",
        "    return scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting single retrain job...\n",
            "[AAPL] Rows available = 1650\n",
            " → Using FULL indicators (RSI14, MA20, MA50)...\n",
            "[AMZN] Rows available = 1650\n",
            " → Using FULL indicators (RSI14, MA20, MA50)...\n",
            "[GOOGL] Rows available = 1650\n",
            " → Using FULL indicators (RSI14, MA20, MA50)...\n",
            "[IBM] Rows available = 1648\n",
            " → Using FULL indicators (RSI14, MA20, MA50)...\n",
            "[META] Rows available = 1650\n",
            " → Using FULL indicators (RSI14, MA20, MA50)...\n",
            "[MSFT] Rows available = 1650\n",
            " → Using FULL indicators (RSI14, MA20, MA50)...\n",
            "[NFLX] Rows available = 1650\n",
            " → Using FULL indicators (RSI14, MA20, MA50)...\n",
            "[NVDA] Rows available = 1650\n",
            " → Using FULL indicators (RSI14, MA20, MA50)...\n",
            "[ORCL] Rows available = 1648\n",
            " → Using FULL indicators (RSI14, MA20, MA50)...\n",
            "[TSLA] Rows available = 1650\n",
            " → Using FULL indicators (RSI14, MA20, MA50)...\n",
            "Final feature-engineered shape: (16006, 16)\n",
            "Feature-engineered dataset shape: (16006, 16)\n",
            "Warning - missing features, dropping: ['return_1', 'return_3', 'return_7', 'sma_5', 'sma_10', 'sma_20', 'ema_10', 'ema_20', 'vol_5', 'vol_10', 'mom_3', 'mom_7', 'vol_change', 'vol_ratio_5', 'rsi_14', 'month', 'dayofweek', 'is_quarter_end', 'close_lag_1', 'close_lag_2', 'close_lag_3', 'close_lag_5', 'vol_lag_1', 'vol_lag_2', 'vol_lag_3', 'vol_lag_5']\n",
            "Models available: ['rf', 'xgb', 'lgbm']\n",
            "\n",
            "Tuning rf ...\n",
            " Best params (rf): {'n_estimators': 200, 'max_depth': 6, 'class_weight': 'balanced'}\n",
            " Validation AUC (rf): 0.4842\n",
            "\n",
            "Tuning xgb ...\n",
            " Best params (xgb): {'subsample': 1.0, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.05}\n",
            " Validation AUC (xgb): 0.4857\n",
            "\n",
            "Tuning lgbm ...\n",
            "[LightGBM] [Info] Number of positive: 6394, number of negative: 6410\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 12804, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499375 -> initscore=-0.002499\n",
            "[LightGBM] [Info] Start training from score -0.002499\n",
            " Best params (lgbm): {'n_estimators': 200, 'max_depth': -1, 'learning_rate': 0.1}\n",
            " Validation AUC (lgbm): 0.5070\n",
            "\n",
            "Stacking: ['lgbm', 'xgb', 'rf']\n",
            "\n",
            "Training stacking model...\n",
            "\n",
            "Stacking Validation AUC: 0.4961\n",
            "\n",
            " Model + scaler saved to MongoDB GridFS\n",
            "Retraining complete.\n",
            "Retrain complete.\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# Run a single retrain now (manual trigger)\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting single retrain job...\")\n",
        "    model, scaler = retrain_from_mongo(days=LOOKBACK_DAYS)\n",
        "    print(\"Retrain complete.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
